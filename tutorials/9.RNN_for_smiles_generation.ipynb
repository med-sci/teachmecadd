{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch import LongTensor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Iterable, List, Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesRNNDataSet(Dataset):\n",
    "\n",
    "    START:str = \"G\"\n",
    "    END:str = \"E\"\n",
    "    PADDING:str = \"A\"\n",
    "\n",
    "    def __init__(self, smiles: Iterable[str])->None:\n",
    "        self.smiles: Iterable[str] = smiles\n",
    "        self.max_len: int = self._max_len\n",
    "        self.alphabet = self._alphabet\n",
    "        self.inv_alphabet = self._inv_alphabet\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.smiles)\n",
    "\n",
    "    @property\n",
    "    def _max_len(self)->int:\n",
    "        return max([len(smile) for smile in self.smiles])\n",
    "\n",
    "    @property\n",
    "    def _alphabet(self)-> Dict[str, int]:\n",
    "        alphabet =  list(set.union(*[set(smile)for smile in self.smiles]))+[self.START, self.END, self.PADDING]\n",
    "        return {element:value for value, element in enumerate(alphabet)}\n",
    "\n",
    "    @property\n",
    "    def _inv_alphabet(self):\n",
    "        return {value:key for key, value in self._alphabet.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        padding_len:int = self.max_len - len(self.smiles[index])\n",
    "        preprocessed:str = self.START + self.smiles[index] + self.END + self.PADDING*padding_len\n",
    "        return LongTensor([self.alphabet[letter] for letter in preprocessed][:-1]), LongTensor([self.alphabet[letter] for letter in preprocessed][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(embed_size, hidden_size)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, character, hidden, cell):\n",
    "        embedded = self.embed(character)\n",
    "        hidden, cell = self.lstm(embedded, (hidden, cell))\n",
    "        out = self.fc(hidden)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_zero_state(self):\n",
    "        hidden = torch.zeros(1, self.hidden_size)\n",
    "        cell = torch.zeros(1, self.hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_data(dataset: Dataset)->Tuple[torch.Tensor]:\n",
    "    return dataset[np.random.randint(0, len(dataset))]\n",
    "\n",
    "def evaluate(model, temperature):\n",
    "    hidden, cell = model.init_zero_state()\n",
    "\n",
    "    inp = torch.tensor([dataset._alphabet[\"G\"]])\n",
    "    predicted = \"G\"\n",
    "\n",
    "    for _ in range(len(dataset[0][0])):\n",
    "        out, hidden, cell = model(inp, hidden, cell)\n",
    "        output_dist = out.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        predicted_char = dataset._inv_alphabet[top_i.item()]\n",
    "        predicted += predicted_char\n",
    "        inp = torch.tensor([dataset._alphabet[predicted_char]])\n",
    "    \n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_smiles(smile):\n",
    "    trimmed_smile = \"\"\n",
    "    for char in smile[1:]:\n",
    "        if char == \"E\":\n",
    "            return trimmed_smile\n",
    "        trimmed_smile+=char    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/androgen_data.csv\")\n",
    "smiles = data[\"canonical_smiles\"].to_list()\n",
    "dataset = SmilesRNNDataSet(smiles)\n",
    "\n",
    "ALPHABET_SIZE = len(dataset._alphabet)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "\n",
    "model = RNN(ALPHABET_SIZE, EMBEDDING_DIM, HIDDEN_DIM, ALPHABET_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0| Loss: 0.07067514955997467\n",
      "[C-]#[N+]c1ccc(N2C(F)(F)F)ccc1O\n",
      "\n",
      "\n",
      "Iteration: 500| Loss: 0.16816778481006622\n",
      "Cc1nn(-c2ccccc2Cl)cc1Cl\n",
      "\n",
      "\n",
      "Iteration: 1000| Loss: 0.15978243947029114\n",
      "C[C@]12CCC3C(CC=C4C[C@@H]4CC(=O)CC[C@@]43CC[C@@H](c4ccc3)C[C@@]21C\n",
      "\n",
      "\n",
      "Iteration: 1500| Loss: 0.36353158950805664\n",
      "C[C@](O)(C#C)[C@H]1CC(C)CCCCCCCC2=Cc3c(-c4ccc(F)cc4)N(C)C)ccc3c3)c2c1-c1ccc(F)c2c1[C@@H](c1ccccc1)C(=O)Nc1ccc(C(=O)O)c(C)c1\n",
      "\n",
      "\n",
      "Iteration: 2000| Loss: 0.09330683201551437\n",
      "None\n",
      "\n",
      "\n",
      "Iteration: 2500| Loss: 0.18971037864685059\n",
      "CNC(=O)c1cccc(C(=O)O)cc1C(F)(F)F\n",
      "\n",
      "\n",
      "Iteration: 3000| Loss: 0.1421380490064621\n",
      "COC(=O)c1c(O)cccc1O\n",
      "\n",
      "\n",
      "Iteration: 3500| Loss: 0.3314056694507599\n",
      "C[C@](O)(C(NC(=O)[C@H]1CC[C@H]1[C@H]2CC[C@]2(C)C(CC[C@@]21C)[C@@]1(C)CCC1=CC[C@@]21C=CCO1\n",
      "\n",
      "\n",
      "Iteration: 4000| Loss: 0.11134600639343262\n",
      "Nc1ccccc1-c1ccc2[nH]c3c(c21)C(=O)c1cc2ccccc2Cl)C(=O)C1CCCC2)c1ccc(Cl)c(Cl)c1\n",
      "\n",
      "\n",
      "Iteration: 4500| Loss: 0.0832793191075325\n",
      "Cc1cc(N2CCC3CC2CC2)cc1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(5000):\n",
    "    hidden, cell = model.init_zero_state()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    losses = []\n",
    "\n",
    "    inputs, targets = get_random_data(dataset)\n",
    "\n",
    "    for char, target in zip(inputs, targets):\n",
    "        out, hidden, cell = model(char.unsqueeze(0), hidden, cell)\n",
    "        loss+= nn.functional.cross_entropy(out, target.view(1))\n",
    "    \n",
    "    loss = loss/len(inputs)\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if iteration % 500 == 0:\n",
    "            print(f\"Iteration: {iteration}| Loss: {loss}\")\n",
    "            print(trim_smiles(evaluate(model, 0.8)))\n",
    "            print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10bb543452b0f064e04097eeef112f065bfd346d07a52f5c26bfdc16270968ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cadd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
