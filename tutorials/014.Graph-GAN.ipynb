{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.conv import TransformerConv, GATConv\n",
    "from torch_geometric.nn import BatchNorm, global_max_pool, Set2Set\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from rdkit import RDLogger\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL) \n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/androgen_data.csv\")\n",
    "smiles = data[\"canonical_smiles\"].to_list()\n",
    "molecules = [Chem.MolFromSmiles(smile) for smile in smiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_nums = []\n",
    "non_unique_bonds = []\n",
    "\n",
    "for molecule in molecules:\n",
    "    \n",
    "    for atom in molecule.GetAtoms():\n",
    "        non_unique_nums.append(atom.GetAtomicNum())\n",
    "    \n",
    "    for bond in molecule.GetBonds():\n",
    "        non_unique_bonds.append(bond.GetBondType())\n",
    "\n",
    "unique_bonds = set(non_unique_bonds)\n",
    "unique_nums = set(non_unique_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_alphabet = {1:\"H\", 6:\"C\", 7:\"N\", 8:\"O\", 9:\"F\",14:\"Si\",16:\"S\", 17:\"Cl\",35:\"Br\", 53:\"I\"}\n",
    "encoded_atom_nums = {initial:encoded for encoded, initial in enumerate(unique_nums)}\n",
    "\n",
    "encoded_atom_bonds = {initial:encoded+1 for encoded, initial in enumerate(unique_bonds)}\n",
    "max_atoms = max([molecule.GetNumAtoms() for molecule in molecules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded atoms:  {1: 0, 35: 1, 6: 2, 7: 3, 8: 4, 9: 5, 14: 6, 16: 7, 17: 8, 53: 9}\n",
      "\n",
      "Encoded bonds:  {rdkit.Chem.rdchem.BondType.SINGLE: 1, rdkit.Chem.rdchem.BondType.DOUBLE: 2, rdkit.Chem.rdchem.BondType.TRIPLE: 3, rdkit.Chem.rdchem.BondType.AROMATIC: 4}\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded atoms: \", encoded_atom_nums)\n",
    "print(\"\\nEncoded bonds: \", encoded_atom_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_matrix(molecule,alphabet):\n",
    "    annotation_matrix = torch.zeros((molecule.GetNumAtoms(), len(alphabet)))\n",
    "    \n",
    "    for i, atom in enumerate(molecule.GetAtoms()):\n",
    "        j = alphabet[atom.GetAtomicNum()]\n",
    "        annotation_matrix[i,j] = 1\n",
    "        \n",
    "    return annotation_matrix.float()\n",
    "\n",
    "\n",
    "def get_edge_index_with_attrs(molecule, alphabet):\n",
    "    matrix: ndarray = Chem.GetAdjacencyMatrix(molecule)\n",
    "    num_atoms = molecule.GetNumAtoms()\n",
    "\n",
    "    for bond in molecule.GetBonds():\n",
    "        i: int = bond.GetBeginAtomIdx()\n",
    "        j: int = bond.GetEndAtomIdx()\n",
    "        \n",
    "        bond_idx = alphabet[bond.GetBondType()]\n",
    "        \n",
    "        matrix[i, j] = bond_idx\n",
    "        matrix[j, i] = bond_idx\n",
    "    \n",
    "    return dense_to_sparse(torch.LongTensor(matrix))\n",
    "\n",
    "\n",
    "def get_torch_data(molecule, atom_alphabet, bond_alphabet):\n",
    "    \n",
    "    edge_index, edge_attr = get_edge_index_with_attrs(molecule, bond_alphabet)\n",
    "    x = get_annotation_matrix(molecule, atom_alphabet)\n",
    "    \n",
    "    return Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [get_torch_data(molecule, encoded_atom_nums, encoded_atom_bonds) for molecule in molecules]\n",
    "dataloader = DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGAN(nn.Module):\n",
    "    def __init__(self, in_channels, max_mol_size, decode_dim, decode_hidden, atom_alphabet_size, bond_alphabet_size):\n",
    "        self.in_channels = in_channels\n",
    "        self.bond_alphabet_size = bond_alphabet_size\n",
    "        self.encoder_embeding_size = 512\n",
    "        self.max_mol_size = max_mol_size\n",
    "        self.decode_dim = decode_dim\n",
    "        self.decode_hidden = decode_hidden\n",
    "        self.annotation_mat_size = atom_alphabet_size*max_mol_size\n",
    "        self.adj_tensor_shape = int(((max_mol_size*(max_mol_size-1))/2)*bond_alphabet_size)\n",
    "        #self.adj_tensor_shape = self.bond_alphabet_size*self.max_mol_size*self.max_mol_size\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(in_channels=self.in_channels, edge_dim=1, out_channels=self.encoder_embeding_size)\n",
    "        self.bn1 = BatchNorm(self.encoder_embeding_size)\n",
    "        self.conv2 = GATConv(in_channels=self.encoder_embeding_size, edge_dim=1, out_channels=self.encoder_embeding_size)\n",
    "        \n",
    "        self.pooling = Set2Set(self.encoder_embeding_size, processing_steps=4)\n",
    "        self.discrim_linear = nn.Linear(self.encoder_embeding_size*2, 1)\n",
    "        \n",
    "        self.gen_linear_1 = nn.Linear(self.decode_dim, self.decode_hidden)\n",
    "        self.gen_linear_2 = nn.Linear(self.decode_hidden, self.decode_hidden)\n",
    "        \n",
    "        self.gen_annotation_mat = nn.Linear(self.decode_hidden, self.annotation_mat_size)\n",
    "        self.gen_upper_adj_tensor = nn.Linear(self.decode_hidden, self.adj_tensor_shape)\n",
    "    \n",
    "    def discriminator(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "        data.x.float(),\n",
    "        data.edge_index.long(),\n",
    "        data.edge_attr.float(),\n",
    "        data.batch\n",
    "    )\n",
    "        z = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        z = self.bn1(z)\n",
    "        z = self.conv2(z, edge_index, edge_attr)\n",
    "        z = self.pooling(z, batch)\n",
    "        z = torch.sigmoid(self.discrim_linear(z))\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def generate_graph(self, z):\n",
    "        z = F.relu(self.gen_linear_1(z))\n",
    "        z = F.relu(self.gen_linear_2(z))\n",
    "        \n",
    "        annotation_matrix = F.gumbel_softmax(self.gen_annotation_mat(z).reshape(self.max_mol_size, self.in_channels), hard=True)\n",
    "        adj_tensor_2d = torch.tanh(self.gen_upper_adj_tensor(z).reshape(4,-1))\n",
    "        return annotation_matrix, adj_tensor_2d\n",
    "    \n",
    "    def generate_batch(self, Z):\n",
    "        atoms_logits = []\n",
    "        edge_logits = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MOLECULE_SIZE = 20\n",
    "noise = torch.randn(512)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "model = GraphGAN(10, 20, 512, 1024, 10, 4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded = model.discriminator(batch)\n",
    "    annotation_matrix, adj_tensor_2d = model.generate_graph(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_mat(max_nodes, triu_vals):\n",
    "    triu_indices = torch.triu_indices(max_nodes,max_nodes,1)\n",
    "\n",
    "    adj_new = torch.zeros(max_nodes, max_nodes)\n",
    "\n",
    "    adj_new[triu_indices[0],triu_indices[1]] = triu_vals\n",
    "    adj_new = adj_new + torch.transpose(adj_new, 0,1)\n",
    "    \n",
    "    return adj_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_tensor_3d(adj_tensor_2d, max_nodes, max_edges):\n",
    "    adj_tensor_3d = torch.empty(max_edges, max_nodes, max_nodes)\n",
    "    \n",
    "    for edge in range(max_edges):\n",
    "        adj_tensor_3d[edge] = get_adj_mat(max_nodes, adj_tensor_2d[edge])\n",
    "    \n",
    "    return adj_tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[20, 10], edge_index=[2, 1520], edge_attr=[1520])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=Data()\n",
    "d.x = annotation_matrix\n",
    "\n",
    "adj_tensor_3d = get_adj_tensor_3d(adj_tensor_2d, 20, 4)\n",
    "d.edge_index, d.edge_attr = dense_to_sparse(adj_tensor_3d)\n",
    "\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62b8a3e757d337d97b634ddda54cb0d3c3e13e0a1eede25a6efb0204e4640e82"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
